{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f3ff293-670a-48ec-bc45-12ab30a7c470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69368873-7ec0-48e6-8f6b-2964a3f4a3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/spark/spark-3.3.2-bin-hadoop3/python/pyspark/__init__.py'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c423c3ea-cee4-4ecf-beb0-49424bd0821a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eee46680-b152-44de-aa16-35c69ee62ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/01 16:41:26 WARN Utils: Your hostname, ubuntu-s-4vcpu-8gb-blr1-01 resolves to a loopback address: 127.0.1.1; using 10.47.0.5 instead (on interface eth0)\n",
      "24/03/01 16:41:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/01 16:41:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "479e2645-8f9f-47a7-9ff5-6500812177be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.2\n"
     ]
    }
   ],
   "source": [
    "# Question 1: Install Spark and PySpark - What's the output?\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6236afc8-3609-4607-a6c2-3ed78f60b22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-01 16:54:31--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz\n",
      "Resolving github.com (github.com)... 20.207.73.82\n",
      "Connecting to github.com (github.com)|20.207.73.82|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/efdfcf82-6d5c-44d1-a138-4e8ea3c3a3b6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240301%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240301T155432Z&X-Amz-Expires=300&X-Amz-Signature=cd584bd59e8e002ddcc0137fd4e1fbcdd43a79597ec542a98e7dd3507a2bb203&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhv_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-03-01 16:54:32--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/efdfcf82-6d5c-44d1-a138-4e8ea3c3a3b6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240301%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240301T155432Z&X-Amz-Expires=300&X-Amz-Signature=cd584bd59e8e002ddcc0137fd4e1fbcdd43a79597ec542a98e7dd3507a2bb203&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhv_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19375751 (18M) [application/octet-stream]\n",
      "Saving to: ‘fhv_tripdata_2019-10.csv.gz’\n",
      "\n",
      "fhv_tripdata_2019-1 100%[===================>]  18.48M  20.1MB/s    in 0.9s    \n",
      "\n",
      "2024-03-01 16:54:33 (20.1 MB/s) - ‘fhv_tripdata_2019-10.csv.gz’ saved [19375751/19375751]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb857e58-fe0f-4680-a4e6-ee160465c08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Try inferring the Schema\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv('fhv_tripdata_2019-10.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "716d5895-ea20-48c7-86af-62170c1850fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c73059c-be9e-435b-bbb1-ea194345143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "060b38a7-108e-4460-8135-ed9d994313e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_path = \"pq\"\n",
    "\n",
    "df\\\n",
    "    .repartition(6) \\\n",
    "    .write.parquet(output_path, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82d3c56d-7f0b-489f-b6da-9c46ae5f5af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mpq\u001b[00m\n",
      "├── _SUCCESS\n",
      "├── part-00000-5c301ca2-a996-4a0b-8dad-475fe0f1cced-c000.snappy.parquet\n",
      "├── part-00001-5c301ca2-a996-4a0b-8dad-475fe0f1cced-c000.snappy.parquet\n",
      "├── part-00002-5c301ca2-a996-4a0b-8dad-475fe0f1cced-c000.snappy.parquet\n",
      "├── part-00003-5c301ca2-a996-4a0b-8dad-475fe0f1cced-c000.snappy.parquet\n",
      "├── part-00004-5c301ca2-a996-4a0b-8dad-475fe0f1cced-c000.snappy.parquet\n",
      "└── part-00005-5c301ca2-a996-4a0b-8dad-475fe0f1cced-c000.snappy.parquet\n",
      "\n",
      "0 directories, 7 files\n"
     ]
    }
   ],
   "source": [
    "!tree pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7bdaf0ab-56df-4ab8-b565-e1769313fe85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39M\tpq\n"
     ]
    }
   ],
   "source": [
    "# Question 2 - What is the average size of the Parquet?\n",
    "!du -h pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dd8769fa-14e8-442e-96bc-787845707d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n",
      "[Stage 35:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|num_rows|\n",
      "+--------+\n",
      "|   62610|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Question 3 - How many taxi trips were there on the 15th of October?\n",
    "df.registerTempTable('trips_data')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    count(1) AS num_rows\n",
    "FROM\n",
    "    trips_data\n",
    "WHERE\n",
    "    pickup_datetime BETWEEN '2019-10-15 00:00:00' AND '2019-10-15 23:59:59'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "747d693c-fcd3-46d9-b229-2d7d2b405549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|       day|   duration_hours|\n",
      "+----------+-----------------+\n",
      "|2019-10-11|         631152.5|\n",
      "|2019-10-28|         631152.5|\n",
      "|2019-10-31|87672.44083333333|\n",
      "|2019-10-01|70128.02805555555|\n",
      "|2019-10-17|           8794.0|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Question 4 - Longest trip for each day. What is the length of the longest trip in the dataset in hours?\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    CAST(pickup_datetime AS DATE) AS day,\n",
    "    --TIMESTAMPDIFF(HOUR, dropOff_datetime, pickup_datetime) AS duration_hours\n",
    "    (UNIX_TIMESTAMP(dropOff_datetime) - UNIX_TIMESTAMP(pickup_datetime)) / 3600 AS duration_hours\n",
    "\n",
    "FROM trips_data\n",
    "\n",
    "GROUP BY \n",
    "day, duration_hours\n",
    "\n",
    "ORDER BY duration_hours DESC\n",
    "\n",
    "LIMIT 5\n",
    "\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3e450cd8-241a-4683-9bd8-e2345d4326ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6 - Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?\n",
    "\n",
    "# Load the zone lookup data into a temp view in Spark\n",
    "\n",
    "zones_df = spark.read.csv('taxi_zone_lookup.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "088d8196-cb83-4763-a813-11e1fa1ded38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create TempView\n",
    "zones_df.registerTempTable('zones_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c5daeaee-7637-4b43-9ea2-6337bc215cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zones_df.printSchema()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e5b1a742-10a6-4973-8f12-e9e9985df2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|num_trips|           Zone_Name|\n",
      "+---------+--------------------+\n",
      "|        1|         Jamaica Bay|\n",
      "|        2|Governor's Island...|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(l.pickup_datetime) AS num_trips, r.Zone AS Zone_Name\n",
    "    FROM trips_data AS l\n",
    "    JOIN zones_data AS r \n",
    "    ON \n",
    "        l.PUlocationID = r.LocationID\n",
    "    GROUP BY Zone_Name\n",
    "    ORDER BY num_trips ASC\n",
    "    LIMIT 2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376d94f8-05c9-4db2-88e0-26436808b530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
